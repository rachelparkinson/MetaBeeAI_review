{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify papers that are not assigned to any domain, and remove papers from final dataset that are not relevant to any domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create file with papers that aren't in any domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 186 'not included' papers to ../output/not_included_papers.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the cleaned merged dataset\n",
    "cleaned_merged_dataset_path = \"../output/cleaned_merged_dataset.csv\"\n",
    "cleaned_merged_df = pd.read_csv(cleaned_merged_dataset_path)\n",
    "\n",
    "# Step 2: Identify papers not included (NaN OR 0) in any domain columns\n",
    "# Fill NaN with 0, then check if everything == 0 across each row from column 10 onward.\n",
    "not_included_mask = (cleaned_merged_df.iloc[:, 9:].fillna(0) == 0).all(axis=1)\n",
    "not_included_df = cleaned_merged_df[not_included_mask].copy()\n",
    "\n",
    "# Step 3: Save the dataset with papers that are not included in any of the reviews\n",
    "not_included_output_path = \"../output/not_included_papers.csv\"\n",
    "not_included_df.to_csv(not_included_output_path, index=False)\n",
    "\n",
    "print(f\"Saved {len(not_included_df)} 'not included' papers to {not_included_output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These papers have been reviewed in ASReview. The resulting dataset is \"relevant_unassigned_papers.csv\"\n",
    "\n",
    "2. Create a dataset that contains only assigned documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 865 'included' papers to ../output/included_papers.csv\n"
     ]
    }
   ],
   "source": [
    "# Identify papers included in at least one domain\n",
    "\n",
    "# Step 1: load the cleaned merged dataset\n",
    "cleaned_merged_dataset_path = \"../output/cleaned_merged_dataset.csv\"\n",
    "cleaned_merged_df = pd.read_csv(cleaned_merged_dataset_path)\n",
    "\n",
    "# Step 2: Identify papers included in at least one domain\n",
    "included_mask = (cleaned_merged_df.iloc[:, 9:].fillna(0) != 0).any(axis=1)\n",
    "included_df = cleaned_merged_df[included_mask].copy()\n",
    "\n",
    "# Step 3: save the dataset with papers that are included in at least one of the reviews\n",
    "included_output_path = \"../output/included_papers.csv\"\n",
    "included_df.to_csv(included_output_path, index=False)\n",
    "\n",
    "print(f\"Saved {len(included_df)} 'included' papers to {included_output_path}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Merge relevant_unassigned_papers and included_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated titles:\n",
      "Series([], Name: count, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Merge ../data/relevant_unassigned_papers.csv and output/included_papers.csv\n",
    "\n",
    "# Step 1: Load the relevant unassigned papers\n",
    "relevant_unassigned_papers_path = \"../data/relevant_unassigned_papers.csv\"\n",
    "relevant_unassigned_df = pd.read_csv(relevant_unassigned_papers_path)\n",
    "\n",
    "# Step 2: Merge the relevant unassigned papers with the included papers\n",
    "merged_df = pd.concat([included_df, relevant_unassigned_df], ignore_index=True)\n",
    "\n",
    "# Step 3: Check for duplicates using the 'title' column (not case sensitive)\n",
    "duplicates_mask = merged_df.duplicated(subset=\"title\", keep=False)\n",
    "duplicates_df = merged_df[duplicates_mask].copy()\n",
    "# Print titles that are duplicated\n",
    "print(\"Duplicated titles:\")\n",
    "print(duplicates_df[\"title\"].value_counts())\n",
    "\n",
    "# Step 4: Save the merged final dataset\n",
    "merged_output_path = \"../output/final_dataset.csv\"\n",
    "merged_df.to_csv(merged_output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for missing DOIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing DOIs: 41\n"
     ]
    }
   ],
   "source": [
    "# Check final_dataset.csv for missing values in doi column\n",
    "final_dataset_path = \"../output/final_dataset.csv\"\n",
    "final_df = pd.read_csv(final_dataset_path)\n",
    "\n",
    "# Check for missing values in the 'doi' column\n",
    "missing_doi_mask = final_df[\"doi\"].isnull()\n",
    "missing_doi_df = final_df[missing_doi_mask].copy()\n",
    "print(f\"Missing DOIs: {len(missing_doi_df)}\")\n",
    "\n",
    "# Save the dataset with missing DOIs\n",
    "missing_doi_output_path = \"../output/missing_doi.csv\"\n",
    "missing_doi_df.to_csv(missing_doi_output_path, index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
